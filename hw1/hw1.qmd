---
title: "Biostat 212a Homework 1"
subtitle: "Due Jan 28, 2025 @ 11:59PM"
author: "Molly Shi (UID: 906558988)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
editor: 
  markdown: 
    wrap: 72
---

## 1 Filling gaps in lecture notes (10% pts)

Consider the regression model $$
Y = f(X) + \epsilon,
$$ where $\operatorname{E}(\epsilon) = 0$.

### 1.1 Optimal regression function

Show that the choice $$
f_{\text{opt}}(X) = \operatorname{E}(Y | X)
$$ minimizes the mean squared prediction error $$
\operatorname{E}\{[Y - f(X)]^2\},
$$ where the expectations averages over variations in both $X$ and $Y$.
(Hint: condition on $X$.)

### 1.2 Bias-variance trade-off

Given an estimate $\hat f$ of $f$, show that the test error at a $x_0$
can be decomposed as $$
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} = \underbrace{\operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2}_{\text{MSE of } \hat f(x_0) \text{ for estimating } f(x_0)} + \underbrace{\operatorname{Var}(\epsilon)}_{\text{irreducible}},
$$ where the expectation averages over the variability in $y_0$ and
$\hat f$.

## 2 ISL Exercise 2.4.3 (10% pts)

We now revisit the bias-variance decomposition.

\(a\) Provide a sketch of typical (squared) bias, variance, training
error, test error, and Bayes (or irreducible) error curves, on a single
plot, as we go from less flexible statistical learning methods towards
more flexible approaches. The x-axis should represent the amount of
flexibility in the method, and the y-axis should represent the values
for each curve. There should be five curves. Make sure to label each
one.

\(b\) Explain why each of the five curves has the shape displayed in
part (a).

```{r, eval = F}
library(tidyverse)
fit <- lm(sales ~ TV, data = )
```

## 3 ISL Exercise 2.4.4 (10% pts)

You will now think of some real-life applications for statistical
learning.

\(a\) Describe three real-life applications in which classification
might be useful. Describe the response, as well as the predictors. Is
the goal of each application inference or prediction? Explain your
answer.

\(b\) Describe three real-life applications in which regression might be
useful. Describe the response, as well as the predictors. Is the goal of
each application inference or prediction? Explain your answer.

\(c\) Describe three real-life applications in which cluster analysis
might be useful.

## 4 ISL Exercise 2.4.10 (30% pts)

This exercise involves the `Boston` housing data set.

Your can read in the `boston` data set directly from url
<https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv>.
A documentation of the `boston` data set is
[here](https://www.rdocumentation.org/packages/ISLR2/versions/1.3-2/topics/Boston).

::: panel-tabset
#### R

```{r, evalue = F}
library(tidyverse)

Boston <- read_csv("https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv", col_select = -1) %>% 
  print(width = Inf)
```

#### Python

```{python}
import pandas as pd
import io
import requests

url = "https://raw.githubusercontent.com/ucla-econ-425t/2023winter/master/slides/data/Boston.csv"
s = requests.get(url).content
Boston = pd.read_csv(io.StringIO(s.decode('utf-8')), index_col = 0)
Boston
```
:::

(a) To begin, load in the Boston data set. The Boston data set is part
    of the ISLR2 library in R.

## 5 ISL Exercise 3.7.3 (20% pts)

## 6 ISL Exercise 3.7.15 (20% pts)

## 7 Bonus question (20% pts)

For multiple linear regression, show that $R^2$ is equal to the
correlation between the response vector
$\mathbf{y} = (y_1, \ldots, y_n)^T$ and the fitted values
$\hat{\mathbf{y}} = (\hat y_1, \ldots, \hat y_n)^T$. That is $$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = [\operatorname{Cor}(\mathbf{y}, \hat{\mathbf{y}})]^2.
$$
